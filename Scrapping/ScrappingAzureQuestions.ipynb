{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://learn.microsoft.com/en-us/answers/tags/133/azure?filterby=withacceptedanswer\n",
      "Error during scraping: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102dc1568 chromedriver + 6088040\n",
      "1   chromedriver                        0x0000000102db917a chromedriver + 6054266\n",
      "2   chromedriver                        0x0000000102858540 chromedriver + 415040\n",
      "3   chromedriver                        0x00000001028aa0a0 chromedriver + 749728\n",
      "4   chromedriver                        0x00000001028aa2f1 chromedriver + 750321\n",
      "5   chromedriver                        0x00000001028fa764 chromedriver + 1079140\n",
      "6   chromedriver                        0x00000001028d041d chromedriver + 906269\n",
      "7   chromedriver                        0x00000001028f7a19 chromedriver + 1067545\n",
      "8   chromedriver                        0x00000001028d01c3 chromedriver + 905667\n",
      "9   chromedriver                        0x000000010289c05a chromedriver + 692314\n",
      "10  chromedriver                        0x000000010289d1b1 chromedriver + 696753\n",
      "11  chromedriver                        0x0000000102d80c90 chromedriver + 5823632\n",
      "12  chromedriver                        0x0000000102d84b44 chromedriver + 5839684\n",
      "13  chromedriver                        0x0000000102d5be86 chromedriver + 5672582\n",
      "14  chromedriver                        0x0000000102d8553b chromedriver + 5842235\n",
      "15  chromedriver                        0x0000000102d4a824 chromedriver + 5601316\n",
      "16  chromedriver                        0x0000000102da7618 chromedriver + 5981720\n",
      "17  chromedriver                        0x0000000102da77df chromedriver + 5982175\n",
      "18  chromedriver                        0x0000000102db8d58 chromedriver + 6053208\n",
      "19  libsystem_pthread.dylib             0x00007ff81c1db253 _pthread_start + 99\n",
      "20  libsystem_pthread.dylib             0x00007ff81c1d6bef thread_start + 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "def setup_selenium():\n",
    "    \"\"\"\n",
    "    Set up Selenium WebDriver with Chrome options.\n",
    "    Returns:\n",
    "        WebDriver: Configured WebDriver instance.\n",
    "    \"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = Service(executable_path=\"/usr/local/bin/chromedriver\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "def fetch_question_details(driver, question_link):\n",
    "    \"\"\"\n",
    "    Fetch the details of a specific question, including its full text and accepted answer.\n",
    "    Args:\n",
    "        driver (WebDriver): Selenium WebDriver instance.\n",
    "        question_link (str): URL of the specific question page.\n",
    "    Returns:\n",
    "        dict: Dictionary containing question details (title, question text, accepted answer).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(question_link)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"thread-title\"))\n",
    "        )\n",
    "        time.sleep(2)  # Additional wait for content to load\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        # Extract question title\n",
    "        title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"No Title\"\n",
    "        \n",
    "        # Extract question text\n",
    "        question_text = soup.find(\"div\", class_=\"thread-body\").get_text(strip=True) if soup.find(\"div\", class_=\"thread-body\") else \"No Question Text\"\n",
    "        \n",
    "        # Extract accepted answer\n",
    "        accepted_answer = soup.find(\"div\", class_=\"accepted-answer\").get_text(strip=True) if soup.find(\"div\", class_=\"accepted-answer\") else \"No Accepted Answer\"\n",
    "        \n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"question_text\": question_text,\n",
    "            \"accepted_answer\": accepted_answer,\n",
    "            \"link\": question_link\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching details from {question_link}: {e}\")\n",
    "        return {\n",
    "            \"title\": \"Error\",\n",
    "            \"question_text\": \"Error\",\n",
    "            \"accepted_answer\": \"Error\",\n",
    "            \"link\": question_link\n",
    "        }\n",
    "\n",
    "def scrape_questions(base_url, output_file=\"azure_questions.json\"):\n",
    "    \"\"\"\n",
    "    Scrape Azure questions from the given URL and save them to a file.\n",
    "    Args:\n",
    "        base_url (str): URL to start scraping from.\n",
    "        output_file (str): Path to save the scraped questions.\n",
    "    \"\"\"\n",
    "    driver = setup_selenium()\n",
    "    all_questions = []\n",
    "    error_links = []\n",
    "    \n",
    "    try:\n",
    "        current_page = 1\n",
    "        while True:\n",
    "            print(f\"Scraping page {current_page}: {base_url}\")\n",
    "            \n",
    "            driver.get(base_url)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"thread-title\"))\n",
    "            )\n",
    "            time.sleep(2)  # Additional wait for content to load\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            \n",
    "            # Find all question links\n",
    "            question_links = soup.find_all(\"a\", class_=\"thread-title\", href=True)\n",
    "            question_links = [f\"https://learn.microsoft.com{link['href']}\" for link in question_links]\n",
    "            \n",
    "            if not question_links:\n",
    "                print(\"No more questions found. Stopping.\")\n",
    "                break\n",
    "            \n",
    "            print(f\"Found {len(question_links)} questions on page {current_page}.\")\n",
    "            \n",
    "            for idx, link in enumerate(question_links, start=1):\n",
    "                print(f\"Scraping question {idx} of {len(question_links)} on page {current_page}: {link}\")\n",
    "                \n",
    "                question_details = fetch_question_details(driver, link)\n",
    "                if question_details[\"title\"] == \"Error\":\n",
    "                    error_links.append(link)\n",
    "                else:\n",
    "                    all_questions.append(question_details)\n",
    "            \n",
    "            # Check for \"Next\" button to navigate to the next page\n",
    "            next_button = soup.find(\"a\", class_=\"next\", href=True)\n",
    "            if next_button:\n",
    "                base_url = f\"https://learn.microsoft.com{next_button['href']}\"\n",
    "                current_page += 1\n",
    "            else:\n",
    "                print(\"No more pages to scrape.\")\n",
    "                break\n",
    "        \n",
    "        # Save all questions to a JSON file\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_questions, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        # Save error links to a file\n",
    "        if error_links:\n",
    "            with open(\"errors.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                for error_link in error_links:\n",
    "                    f.write(f\"{error_link}\\n\")\n",
    "        \n",
    "        print(f\"Scraped {len(all_questions)} questions successfully.\")\n",
    "        if error_links:\n",
    "            print(f\"Encountered {len(error_links)} errors. Links saved to errors.txt.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_questions(\"https://learn.microsoft.com/en-us/answers/tags/133/azure?filterby=withacceptedanswer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
